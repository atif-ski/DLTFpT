{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNLAs/P28MKJEGplxbyBm+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atif-ski/DLTFpT/blob/master/SentenceTransfrmr_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIkkUaFnaDEM",
        "outputId": "119d9d5b-025c-4d36-d018-54a6660ac420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=be7d6cc3678528327cb038ef3b5da6225c70ed690d39553daad8cb9b937745af\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl\n",
        "!pip install docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7E5lSMGc7ZT",
        "outputId": "5ff6d982-bbb5-4076-9a53-a9df3feafa2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from docx) (4.9.3)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.10/dist-packages (from docx) (9.4.0)\n",
            "Building wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53898 sha256=7e4e060c188b4f17593833d7c1c0f2b2fe506a7d44d24a7eee7f35b0a511c5c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/f5/1d/e09ba2c1907a43a4146d1189ae4733ca1a3bfe27ee39507767\n",
            "Successfully built docx\n",
            "Installing collected packages: docx\n",
            "Successfully installed docx-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#Our sentences we like to encode\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.',\n",
        "    'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "#Sentences are encoded by calling model.encode()\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "CGwgGs5GaENe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtIFtXK3dX-W",
        "outputId": "51e38ece-8461-4c84-e2b6-57933a3187d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: docx in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from docx) (4.9.3)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.10/dist-packages (from docx) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentence_transformers as st\n",
        "import openpyxl\n",
        "#import docx\n",
        "import codecs\n",
        "#from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compare_texts(excel_text, document_text):\n",
        "  \"\"\"Compares two texts and returns the similarity score.\"\"\"\n",
        "\n",
        "  # Load the sentence-transformers model.\n",
        "  model = st.SentenceTransformer('distilbert-base-uncased')\n",
        "\n",
        "  # Read the Excel text.\n",
        "\n",
        "  #excel_text = 'Knowledge of design, development, and implementation of large-scale scientific data storage, access, retrieval, and mining systems or techniques.Capabilities to transfer research algorithms into processing code that produces scientific data products for the science community.Knowledge of image methods and procedures for automated feature extraction from large data sets.'\n",
        "\n",
        "  # Read the document text from a text file.\n",
        "  with open(document_file, 'r', encoding='windows-1252') as f:\n",
        "    document_text = f.read()\n",
        "\n",
        "  document_text_crctd = document_text.replace(u\"0\\xa0\", \"\")\n",
        "\n",
        "  # Encode the two texts.\n",
        "  excel_embedding = model.encode(excel_text)\n",
        "  document_embedding = model.encode(document_text_crctd)\n",
        "\n",
        "  # Calculate the cosine similarity between the two embeddings.\n",
        "  similarity_score = st.util.cos_sim(excel_embedding, document_embedding)\n",
        "\n",
        "  return similarity_score"
      ],
      "metadata": {
        "id": "4olEgkTqbsyp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentence_transformers as st\n",
        "import openpyxl\n",
        "#import docx\n",
        "import codecs\n",
        "#from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compare_texts_bert_mean(excel_text, document_text):\n",
        "  \"\"\"Compares two texts and returns the similarity score.\"\"\"\n",
        "\n",
        "  # Load the sentence-transformers model.\n",
        "  model = st.SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  # Read the Excel text.\n",
        "\n",
        "  #excel_text = 'Knowledge of design, development, and implementation of large-scale scientific data storage, access, retrieval, and mining systems or techniques.Capabilities to transfer research algorithms into processing code that produces scientific data products for the science community.Knowledge of image methods and procedures for automated feature extraction from large data sets.'\n",
        "\n",
        "  # Read the document text from a text file.\n",
        "  with open(document_file, 'r', encoding='windows-1252') as f:\n",
        "    document_text = f.read()\n",
        "\n",
        "  document_text_crctd = document_text.replace(u\"0\\xa0\", \"\")\n",
        "\n",
        "  # Encode the two texts.\n",
        "  excel_embedding = model.encode(excel_text)\n",
        "  document_embedding = model.encode(document_text_crctd)\n",
        "\n",
        "  # Calculate the cosine similarity between the two embeddings.\n",
        "  similarity_score = st.util.cos_sim(excel_embedding, document_embedding)\n",
        "\n",
        "  return similarity_score"
      ],
      "metadata": {
        "id": "iFGPa-rndPVO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentence_transformers as st\n",
        "import openpyxl\n",
        "#import docx\n",
        "import codecs\n",
        "#from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compare_texts_roberta_mean(excel_text, document_text):\n",
        "  \"\"\"Compares two texts and returns the similarity score.\"\"\"\n",
        "\n",
        "  # Load the sentence-transformers model.\n",
        "  model = st.SentenceTransformer('roberta-base-nli-mean-tokens')\n",
        "\n",
        "  # Read the Excel text.\n",
        "\n",
        "  #excel_text = 'Knowledge of design, development, and implementation of large-scale scientific data storage, access, retrieval, and mining systems or techniques.Capabilities to transfer research algorithms into processing code that produces scientific data products for the science community.Knowledge of image methods and procedures for automated feature extraction from large data sets.'\n",
        "\n",
        "  # Read the document text from a text file.\n",
        "  with open(document_file, 'r', encoding='windows-1252') as f:\n",
        "    document_text = f.read()\n",
        "\n",
        "  document_text_crctd = document_text.replace(u\"0\\xa0\", \"\")\n",
        "\n",
        "  # Encode the two texts.\n",
        "  excel_embedding = model.encode(excel_text)\n",
        "  document_embedding = model.encode(document_text_crctd)\n",
        "\n",
        "  # Calculate the cosine similarity between the two embeddings.\n",
        "  similarity_score = st.util.cos_sim(excel_embedding, document_embedding)\n",
        "\n",
        "  return similarity_score"
      ],
      "metadata": {
        "id": "Sb4910Otdch9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentence_transformers as st\n",
        "import openpyxl\n",
        "#import docx\n",
        "import codecs\n",
        "#from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compare_texts_stsb_mean(excel_text, document_text):\n",
        "  \"\"\"Compares two texts and returns the similarity score.\"\"\"\n",
        "\n",
        "  # Load the sentence-transformers model.\n",
        "  model = st.SentenceTransformer('stsb-bert-base')\n",
        "\n",
        "  # Read the Excel text.\n",
        "\n",
        "  #excel_text = 'Knowledge of design, development, and implementation of large-scale scientific data storage, access, retrieval, and mining systems or techniques.Capabilities to transfer research algorithms into processing code that produces scientific data products for the science community.Knowledge of image methods and procedures for automated feature extraction from large data sets.'\n",
        "\n",
        "  # Read the document text from a text file.\n",
        "  with open(document_file, 'r', encoding='windows-1252') as f:\n",
        "    document_text = f.read()\n",
        "\n",
        "  document_text_crctd = document_text.replace(u\"0\\xa0\", \"\")\n",
        "\n",
        "  # Encode the two texts.\n",
        "  excel_embedding = model.encode(excel_text)\n",
        "  document_embedding = model.encode(document_text_crctd)\n",
        "\n",
        "  # Calculate the cosine similarity between the two embeddings.\n",
        "  similarity_score = st.util.cos_sim(excel_embedding, document_embedding)\n",
        "\n",
        "  return similarity_score"
      ],
      "metadata": {
        "id": "LhTOFulWdrvt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel(\"/content/sample_data/NASA_competency_v4.xlsx\")\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgfPsn5I11U9",
        "outputId": "bc6d734b-fee9-48a8-c170-68daaa2e8072"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       CompDesg                                    CompDescription\n",
            "0       ACQPLAN  Knowledge of contract regulations and governme...\n",
            "1     CONFORMAT  Knowledge of contract regulations and governme...\n",
            "2    CONPERFORM  Knowledge and capabilities associated with man...\n",
            "3      COTRSUB1  Knowledge of the steps required to plan and im...\n",
            "4      COTRSUB2  Ability to monitor contract technical, cost, a...\n",
            "..          ...                                                ...\n",
            "292    SPACEPHY  Knowledge and capabilities to conduct experime...\n",
            "293   ASTRONOMY  Knowledge of fundamental processes of radiatio...\n",
            "294   PLANETSCI  Knowledge of space science applied to conducti...\n",
            "295    ASTROBIO  Knowledge and capabilities of biology, chemist...\n",
            "296  ASTROMATER  Knowledge and capabilities to apply knowledge ...\n",
            "\n",
            "[297 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##excluded_keywords = [\"knowledge\", \"capabilities\", \"practices\", \"tools\", \"these\", \"procedures\", \"develop\"]"
      ],
      "metadata": {
        "id": "infJnonAJggx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "T9pIHoTbd9zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_file = '/content/sample_data/00007.txt'\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "  # Read the Excel file\n",
        "df = pd.read_excel(\"/content/sample_data/NASA_competency_v4.xlsx\")\n",
        "\n",
        "\n",
        "\n",
        "for val in df['CompDescription']:\n",
        "\n",
        "    #Calculate the similarity score.\n",
        "    similarity_score = compare_texts( val ,document_file)\n",
        "    df['similarity_score'] = similarity_score\n",
        "\n",
        "    # Print the similarity score.\n",
        "    #print(f'{val} :::: \\n The similarity score is {similarity_score}')\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "mn31BPIz6zsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "file = ['/content/00323.txt','/content/06728.txt', '/content/24929.txt', '/content/06347.txt', '/content/08890.txt', '/content/26729.txt','/content/28634.txt', '/content/10358.txt', '/content/02626.txt', '/content/28642.txt', '/content/22509.txt','/content/07816.txt', '/content/08217.txt', '/content/28990.txt', '/content/19036.txt', '/content/21950.txt','/content/09253.txt', '/content/21343.txt', '/content/25526.txt', '/content/19635.txt']\n",
        "\n",
        "df_all = pd.DataFrame()\n",
        "# Read the Excel file\n",
        "df = pd.read_excel(\"/content/sample_data/NASA_competency_v4.xlsx\")\n",
        "\n",
        "#df = old_df.iloc[[0]].copy()\n",
        "#print(df)\n",
        "# Create an empty list to store the cosine similarity scores\n",
        "\n",
        "for document_file in file:\n",
        "    cosine_scores = []\n",
        "\n",
        "    # Iterate through each row in the dataframe and calculate cosine similarity\n",
        "    for index, row in df.iterrows():\n",
        "        sentence = row['CompDescription']\n",
        "        # print(sentence)\n",
        "        # sentence_vector = model.encode(sentence, convert_to_tensor=True)\n",
        "        similarity_score = compare_texts(sentence, document_file)\n",
        "        # print(similarity_score)\n",
        "        cosine_scores.append(similarity_score.item())\n",
        "\n",
        "        # if index > 15:\n",
        "        # break\n",
        "\n",
        "    # Add the cosine similarity scores to the dataframe as a new column\n",
        "    df['Cosine Similarity'] = cosine_scores\n",
        "\n",
        "    # Create an empty list to store the cosine similarity scores\n",
        "    cosine_scores_bert = []\n",
        "\n",
        "    # Iterate through each row in the dataframe and calculate cosine similarity\n",
        "    for index, row in df.iterrows():\n",
        "        sentence = row['CompDescription']\n",
        "        # print(sentence)\n",
        "        # sentence_vector = model.encode(sentence, convert_to_tensor=True)\n",
        "        similarity_score = compare_texts_bert_mean(sentence, document_file)\n",
        "        # print(similarity_score)\n",
        "        cosine_scores_bert.append(similarity_score.item())\n",
        "\n",
        "        # if index > 15:\n",
        "        # break\n",
        "\n",
        "    # Add the cosine similarity scores to the dataframe as a new column\n",
        "    df['Cosine_bert'] = cosine_scores_bert\n",
        "\n",
        "    #####################\n",
        "    # Create an empty list to store the cosine similarity scores\n",
        "    cosine_scores_roberta = []\n",
        "\n",
        "    # Iterate through each row in the dataframe and calculate cosine similarity\n",
        "    for index, row in df.iterrows():\n",
        "        sentence = row['CompDescription']\n",
        "        # print(sentence)\n",
        "        # sentence_vector = model.encode(sentence, convert_to_tensor=True)\n",
        "        similarity_score = compare_texts_roberta_mean(sentence, document_file)\n",
        "        # print(similarity_score)\n",
        "        cosine_scores_roberta.append(similarity_score.item())\n",
        "\n",
        "        # if index > 15:\n",
        "        # break\n",
        "\n",
        "    # Add the cosine similarity scores to the dataframe as a new column\n",
        "    df['Cosine_roberta'] = cosine_scores_roberta\n",
        "\n",
        "    ################\n",
        "\n",
        "    # Create an empty list to store the cosine similarity scores\n",
        "    cosine_scores_stsb = []\n",
        "\n",
        "    # Iterate through each row in the dataframe and calculate cosine similarity\n",
        "    for index, row in df.iterrows():\n",
        "        sentence = row['CompDescription']\n",
        "        # print(sentence)\n",
        "        # sentence_vector = model.encode(sentence, convert_to_tensor=True)\n",
        "\n",
        "        similarity_score = compare_texts_stsb_mean(sentence, document_file)\n",
        "        # print(similarity_score)\n",
        "        cosine_scores_stsb.append(similarity_score.item())\n",
        "\n",
        "        # if index > 15:\n",
        "        # break\n",
        "\n",
        "    # Add the cosine similarity scores to the dataframe as a new column\n",
        "    df['Cosine_stsb'] = cosine_scores_stsb\n",
        "\n",
        "    repeated_values = np.full(len(df), document_file)\n",
        "\n",
        "    # Assign the repeated values to the new column in the DataFrame\n",
        "    df['resume'] = repeated_values\n",
        "\n",
        "    #print(df)\n",
        "\n",
        "    df_all = pd.concat([df_all, df], ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnoL3dStT0Vz",
        "outputId": "b8e10f17-17e6-45e6-c8ea-034df44f86b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kAlkkaNLp2bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/sample_data/data_all.xlsx'  # Replace 'data.xlsx' with your desired file name and path\n",
        "\n",
        "# Export the DataFrame to Excel\n",
        "df_all.to_excel(file_path, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "g9qXNKKguqN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 5 rows based on cosine similarity\n",
        "top_5_rows = df.nlargest(10, 'Cosine_bert')\n",
        "\n",
        "# Print the top 5 rows\n",
        "print(top_5_rows)"
      ],
      "metadata": {
        "id": "TWb99JvbYVOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 5 rows based on cosine similarity\n",
        "top_5_rows = df.nlargest(10, 'Cosine_stsb')\n",
        "\n",
        "# Print the top 5 rows\n",
        "print(top_5_rows)"
      ],
      "metadata": {
        "id": "uGKzfZMMuyEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GrgWxmSYYU6j"
      }
    }
  ]
}